{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "def load_json_gz(filename):\n",
    "    with gzip.open(filename, 'r') as f:\n",
    "        i = 0\n",
    "        ret = []\n",
    "        for json_line in f:\n",
    "            if i == 10000:\n",
    "                return ret\n",
    "            data = json.loads(json_line)\n",
    "            text = data['text']\n",
    "            if len(text) > 2000:\n",
    "                ret.append(text)\n",
    "                i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now download the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 10000 strings from C4 dataset: https://huggingface.co/datasets/allenai/c4/tree/main/en\n",
    "strings = load_json_gz('c4-train.00000-of-01024.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mamba_out/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_task(batch_size=64, batches=10, model=None, tokenizer=None, token_max_len=25, shuffle=False):\n",
    "    string_idx = 0\n",
    "    success_copies = 0\n",
    "    for _ in range(batches):\n",
    "        cur_batch = []\n",
    "        for count in range(batch_size):\n",
    "            cur_batch.append(strings[count + string_idx])\n",
    "        input_ids = tokenizer(cur_batch, return_tensors=\"pt\", truncation=True, max_length=token_max_len).to(device)[\"input_ids\"]\n",
    "        if shuffle:\n",
    "            col_perm = torch.randperm(input_ids.size(1))\n",
    "            input_ids = input_ids[:, col_perm]\n",
    "        input_ids = torch.cat([input_ids, input_ids], dim=1)\n",
    "        input_ids = torch.cat([input_ids, input_ids[:, 0:1]], dim=1)\n",
    "        output_ids = model.generate(input_ids, max_new_tokens = token_max_len-1)\n",
    "        for count in range(batch_size):\n",
    "            gold_token_len = (input_ids.shape[1]-1) // 2\n",
    "            if torch.equal(input_ids[count][:gold_token_len], output_ids[count][gold_token_len*2:]):\n",
    "                success_copies += 1\n",
    "        string_idx += batch_size\n",
    "    return success_copies / (batch_size * batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:23<00:00, 27.88s/it]\n"
     ]
    }
   ],
   "source": [
    "mamba_370m_tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-2.8b-hf\")\n",
    "mamba_370m_model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-2.8b-hf\")\n",
    "mamba_370m_model.to(device)\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.52s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m [\u001b[39m25\u001b[39m, \u001b[39m50\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m150\u001b[39m, \u001b[39m200\u001b[39m, \u001b[39m250\u001b[39m]:\n\u001b[1;32m      8\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m----> 9\u001b[0m     ans \u001b[39m=\u001b[39m copy_task(batch_size \u001b[39m=\u001b[39;49m \u001b[39m8\u001b[39;49m, model \u001b[39m=\u001b[39;49m mamba_370m_model, tokenizer \u001b[39m=\u001b[39;49m mamba_370m_tokenizer, token_max_len\u001b[39m=\u001b[39;49mi)\n\u001b[1;32m     10\u001b[0m     end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     11\u001b[0m     result_370m_time\u001b[39m.\u001b[39mappend(end_time \u001b[39m-\u001b[39m start_time)\n",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m, in \u001b[0;36mcopy_task\u001b[0;34m(batch_size, batches, model, tokenizer, token_max_len, shuffle)\u001b[0m\n\u001b[1;32m     12\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([input_ids, input_ids], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([input_ids, input_ids[:, \u001b[39m0\u001b[39m:\u001b[39m1\u001b[39m]], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m output_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids, max_new_tokens \u001b[39m=\u001b[39;49m token_max_len\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m count \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_size):\n\u001b[1;32m     16\u001b[0m     gold_token_len \u001b[39m=\u001b[39m (input_ids\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/transformers/generation/utils.py:1531\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massisted_decoding(\n\u001b[1;32m   1515\u001b[0m         input_ids,\n\u001b[1;32m   1516\u001b[0m         candidate_generator\u001b[39m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1527\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1528\u001b[0m     )\n\u001b[1;32m   1529\u001b[0m \u001b[39mif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1530\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1531\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_greedy_search(\n\u001b[1;32m   1532\u001b[0m         input_ids,\n\u001b[1;32m   1533\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mprepared_logits_processor,\n\u001b[1;32m   1534\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mprepared_stopping_criteria,\n\u001b[1;32m   1535\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1536\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1537\u001b[0m         output_logits\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_logits,\n\u001b[1;32m   1538\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1539\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1540\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1541\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1542\u001b[0m     )\n\u001b[1;32m   1544\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1545\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/transformers/generation/utils.py:2436\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2433\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2435\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2436\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2437\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2438\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2439\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2440\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2441\u001b[0m )\n\u001b[1;32m   2443\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2444\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/transformers/models/mamba/modeling_mamba.py:661\u001b[0m, in \u001b[0;36mMambaForCausalLM.forward\u001b[0;34m(self, input_ids, inputs_embeds, cache_params, labels, output_hidden_states, return_dict, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    659\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 661\u001b[0m mamba_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(\n\u001b[1;32m    662\u001b[0m     input_ids,\n\u001b[1;32m    663\u001b[0m     cache_params\u001b[39m=\u001b[39;49mcache_params,\n\u001b[1;32m    664\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    665\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    666\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    667\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    668\u001b[0m )\n\u001b[1;32m    669\u001b[0m hidden_states \u001b[39m=\u001b[39m mamba_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    671\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdtype))\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/transformers/models/mamba/modeling_mamba.py:563\u001b[0m, in \u001b[0;36mMambaModel.forward\u001b[0;34m(self, input_ids, inputs_embeds, cache_params, use_cache, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(mixer_block\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m, hidden_states, cache_params)\n\u001b[1;32m    562\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 563\u001b[0m     hidden_states \u001b[39m=\u001b[39m mixer_block(hidden_states, cache_params\u001b[39m=\u001b[39;49mcache_params)\n\u001b[1;32m    565\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    566\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/transformers/models/mamba/modeling_mamba.py:328\u001b[0m, in \u001b[0;36mMambaBlock.forward\u001b[0;34m(self, hidden_states, cache_params)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresidual_in_fp32:\n\u001b[1;32m    326\u001b[0m     residual \u001b[39m=\u001b[39m residual\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m--> 328\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmixer(hidden_states, cache_params\u001b[39m=\u001b[39;49mcache_params)\n\u001b[1;32m    329\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    330\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/transformers/models/mamba/modeling_mamba.py:292\u001b[0m, in \u001b[0;36mMambaMixer.forward\u001b[0;34m(self, hidden_states, cache_params)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states, cache_params: Optional[MambaCache] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    291\u001b[0m     \u001b[39mif\u001b[39;00m is_fast_path_available \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_proj\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype:\n\u001b[0;32m--> 292\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcuda_kernels_forward(hidden_states, cache_params)\n\u001b[1;32m    293\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslow_forward(hidden_states, cache_params)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/transformers/models/mamba/modeling_mamba.py:191\u001b[0m, in \u001b[0;36mMambaMixer.cuda_kernels_forward\u001b[0;34m(self, hidden_states, cache_params)\u001b[0m\n\u001b[1;32m    189\u001b[0m time_proj_bias \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdt_proj\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mfloat() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdt_proj, \u001b[39m\"\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[39mif\u001b[39;00m cache_params \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m cache_params\u001b[39m.\u001b[39mseqlen_offset \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 191\u001b[0m     scan_outputs \u001b[39m=\u001b[39m selective_state_update(\n\u001b[1;32m    192\u001b[0m         cache_params\u001b[39m.\u001b[39;49mssm_states[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer_idx],\n\u001b[1;32m    193\u001b[0m         hidden_states[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m, \u001b[39m0\u001b[39;49m],\n\u001b[1;32m    194\u001b[0m         discrete_time_step[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m, \u001b[39m0\u001b[39;49m],\n\u001b[1;32m    195\u001b[0m         A,\n\u001b[1;32m    196\u001b[0m         B[:, \u001b[39m0\u001b[39;49m],\n\u001b[1;32m    197\u001b[0m         C[:, \u001b[39m0\u001b[39;49m],\n\u001b[1;32m    198\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mD,\n\u001b[1;32m    199\u001b[0m         gate[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m, \u001b[39m0\u001b[39;49m],\n\u001b[1;32m    200\u001b[0m         time_proj_bias,\n\u001b[1;32m    201\u001b[0m         dt_softplus\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    202\u001b[0m     )\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    203\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     scan_outputs, ssm_state \u001b[39m=\u001b[39m selective_scan_fn(\n\u001b[1;32m    205\u001b[0m         hidden_states,\n\u001b[1;32m    206\u001b[0m         discrete_time_step,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m         return_last_state\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/mamba_ssm/ops/triton/selective_state_update.py:137\u001b[0m, in \u001b[0;36mselective_state_update\u001b[0;34m(state, x, dt, A, B, C, D, z, dt_bias, dt_softplus)\u001b[0m\n\u001b[1;32m    131\u001b[0m BLOCK_SIZE_M, num_warps \u001b[39m=\u001b[39m ((\u001b[39m32\u001b[39m, \u001b[39m4\u001b[39m) \u001b[39mif\u001b[39;00m dstate \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m16\u001b[39m\n\u001b[1;32m    132\u001b[0m                            \u001b[39melse\u001b[39;00m ((\u001b[39m16\u001b[39m, \u001b[39m4\u001b[39m) \u001b[39mif\u001b[39;00m dstate \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m32\u001b[39m \u001b[39melse\u001b[39;00m\n\u001b[1;32m    133\u001b[0m                                  ((\u001b[39m8\u001b[39m, \u001b[39m4\u001b[39m) \u001b[39mif\u001b[39;00m dstate \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m64\u001b[39m \u001b[39melse\u001b[39;00m\n\u001b[1;32m    134\u001b[0m                                   ((\u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m) \u001b[39mif\u001b[39;00m dstate \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m128\u001b[39m \u001b[39melse\u001b[39;00m\n\u001b[1;32m    135\u001b[0m                                    ((\u001b[39m4\u001b[39m, \u001b[39m8\u001b[39m))))))\n\u001b[1;32m    136\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(x\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mindex):\n\u001b[0;32m--> 137\u001b[0m     _selective_scan_update_kernel[grid](\n\u001b[1;32m    138\u001b[0m         state, x, dt, dt_bias, A, B, C, D, z, out,\n\u001b[1;32m    139\u001b[0m         batch, dim, dstate,\n\u001b[1;32m    140\u001b[0m         state\u001b[39m.\u001b[39;49mstride(\u001b[39m0\u001b[39;49m), state\u001b[39m.\u001b[39;49mstride(\u001b[39m1\u001b[39;49m), state\u001b[39m.\u001b[39;49mstride(\u001b[39m2\u001b[39;49m),\n\u001b[1;32m    141\u001b[0m         x\u001b[39m.\u001b[39;49mstride(\u001b[39m0\u001b[39;49m), x\u001b[39m.\u001b[39;49mstride(\u001b[39m1\u001b[39;49m),\n\u001b[1;32m    142\u001b[0m         dt\u001b[39m.\u001b[39;49mstride(\u001b[39m0\u001b[39;49m), dt\u001b[39m.\u001b[39;49mstride(\u001b[39m1\u001b[39;49m),\n\u001b[1;32m    143\u001b[0m         dt_bias\u001b[39m.\u001b[39;49mstride(\u001b[39m0\u001b[39;49m) \u001b[39mif\u001b[39;49;00m dt_bias \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m    144\u001b[0m         A\u001b[39m.\u001b[39;49mstride(\u001b[39m0\u001b[39;49m), A\u001b[39m.\u001b[39;49mstride(\u001b[39m1\u001b[39;49m),\n\u001b[1;32m    145\u001b[0m         B\u001b[39m.\u001b[39;49mstride(\u001b[39m0\u001b[39;49m), B\u001b[39m.\u001b[39;49mstride(\u001b[39m1\u001b[39;49m),\n\u001b[1;32m    146\u001b[0m         C\u001b[39m.\u001b[39;49mstride(\u001b[39m0\u001b[39;49m), C\u001b[39m.\u001b[39;49mstride(\u001b[39m1\u001b[39;49m),\n\u001b[1;32m    147\u001b[0m         D\u001b[39m.\u001b[39;49mstride(\u001b[39m0\u001b[39;49m) \u001b[39mif\u001b[39;49;00m D \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m    148\u001b[0m         z_strides[\u001b[39m0\u001b[39;49m], z_strides[\u001b[39m1\u001b[39;49m],\n\u001b[1;32m    149\u001b[0m         out\u001b[39m.\u001b[39;49mstride(\u001b[39m0\u001b[39;49m), out\u001b[39m.\u001b[39;49mstride(\u001b[39m1\u001b[39;49m),\n\u001b[1;32m    150\u001b[0m         dt_softplus,\n\u001b[1;32m    151\u001b[0m         BLOCK_SIZE_M,\n\u001b[1;32m    152\u001b[0m         num_warps\u001b[39m=\u001b[39;49mnum_warps,\n\u001b[1;32m    153\u001b[0m     )\n\u001b[1;32m    154\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "result_370m_time = []\n",
    "result_370m = []\n",
    "for i in [25, 50, 100, 150, 200, 250]:\n",
    "    start_time = time.time()\n",
    "    ans = copy_task(batch_size = 8, model = mamba_370m_model, tokenizer = mamba_370m_tokenizer, token_max_len=i)\n",
    "    end_time = time.time()\n",
    "    result_370m_time.append(end_time - start_time)\n",
    "    result_370m.append(ans)\n",
    "    print(ans)\n",
    "    print('the time spent is', end_time - start_time)\n",
    "\n",
    "print(\"the result 370m time is \", result_370m_time)\n",
    "print(\"the result 370m is \", result_370m)\n",
    "# print(copy_task(model=mamba_370m_model, tokenizer=mamba_370m_tokenizer, token_max_len=25))\n",
    "# print(copy_task(model=mamba_370m_model, tokenizer=mamba_370m_tokenizer, token_max_len=50))\n",
    "# print(copy_task(model=mamba_370m_model, tokenizer=mamba_370m_tokenizer, token_max_len=100))\n",
    "# print(copy_task(model=mamba_370m_model, tokenizer=mamba_370m_tokenizer, token_max_len=150))\n",
    "# print(copy_task(model=mamba_370m_model, tokenizer=mamba_370m_tokenizer, token_max_len=200))\n",
    "# print(copy_task(model=mamba_370m_model, tokenizer=mamba_370m_tokenizer, token_max_len=250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9375\n",
      "the time spent is 9.845363855361938\n",
      "0.55\n",
      "the time spent is 19.699750900268555\n",
      "0.075\n",
      "the time spent is 40.13555192947388\n",
      "0.0\n",
      "the time spent is 60.27432703971863\n",
      "0.0\n",
      "the time spent is 80.2201087474823\n",
      "0.0\n",
      "the time spent is 100.15279531478882\n"
     ]
    }
   ],
   "source": [
    "result_370m_shuffle_time = []\n",
    "result_370m_shuffle = []\n",
    "for i in [25, 50, 100, 150, 200, 250]:\n",
    "    start_time = time.time()\n",
    "    ans = copy_task(batch_size = 8, model = mamba_370m_model, tokenizer = mamba_370m_tokenizer, token_max_len=i, shuffle=True)\n",
    "    end_time = time.time()\n",
    "    result_370m_shuffle_time.append(end_time - start_time)\n",
    "    result_370m_shuffle.append(ans)\n",
    "    print(ans)\n",
    "    print('the time spent is', end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shuffle 370m results [0.9375, 0.55, 0.075, 0.0, 0.0, 0.0]\n",
      "the shuffle 370 time [9.845363855361938, 19.699750900268555, 40.13555192947388, 60.27432703971863, 80.2201087474823, 100.15279531478882]\n"
     ]
    }
   ],
   "source": [
    "print(\"the shuffle 370m results\", result_370m_shuffle)\n",
    "print(\"the shuffle 370 time\", result_370m_shuffle_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857f5f36675a4708ace203ea42f7414a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n",
      "the time spent is 4.088669776916504\n",
      "0.85\n",
      "the time spent is 8.237766981124878\n",
      "0.825\n",
      "the time spent is 16.61814284324646\n",
      "0.65\n",
      "the time spent is 24.92847204208374\n",
      "0.45\n",
      "the time spent is 33.319599628448486\n",
      "0.3\n",
      "the time spent is 40.86095356941223\n",
      "the result 1.4b time is  [4.088669776916504, 8.237766981124878, 16.61814284324646, 24.92847204208374, 33.319599628448486, 40.86095356941223]\n",
      "the result 1.4b  [0.95, 0.85, 0.825, 0.65, 0.45, 0.3]\n"
     ]
    }
   ],
   "source": [
    "mamba_1_4b_tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-1.4b-hf\")\n",
    "mamba_1_4b_model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-1.4b-hf\")\n",
    "mamba_1_4b_model.to(device)\n",
    "\n",
    "import time \n",
    "result_1_4b_time = []\n",
    "result_1_4b = []\n",
    "for i in [25, 50, 100, 150, 200, 250]:\n",
    "    start_time = time.time()\n",
    "    ans = copy_task(batch_size = 4, model = mamba_1_4b_model, tokenizer = mamba_1_4b_tokenizer, token_max_len=i)\n",
    "    end_time = time.time()\n",
    "    result_1_4b_time.append(end_time - start_time)\n",
    "    result_1_4b.append(ans)\n",
    "    print(ans)\n",
    "    print('the time spent is', end_time - start_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the result 1.4b time is  [4.088669776916504, 8.237766981124878, 16.61814284324646, 24.92847204208374, 33.319599628448486, 40.86095356941223]\n",
      "the result 1.4b  [0.95, 0.85, 0.825, 0.65, 0.45, 0.3]\n"
     ]
    }
   ],
   "source": [
    "print(\"the result 1.4b time is \", result_1_4b_time)\n",
    "print(\"the result 1.4b \", result_1_4b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b7ba9773374997a4ecade93909d83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0536ba8e82c547579979cc5cc605e1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0b32ffdb554642b1ff78a4a232e71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3353f1421e74c98ae1d2c26b43d811b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/50.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f75d734682140fcbb11c35aee58f56c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48bdaa0b26c401f89e0c4dfcfd34b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1d8722068343abb514e4fdde5032ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b933fc9e9ea49288ca5d3ad21ec0606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/1.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55b52206a8d45ac826fa04259609099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcc701b3c3a44c49481f4b60a2fa566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n",
      "the time spent is 5.494949817657471\n",
      "0.975\n",
      "the time spent is 11.137060403823853\n",
      "0.9\n",
      "the time spent is 22.60603427886963\n",
      "0.825\n",
      "the time spent is 33.62363815307617\n",
      "0.7\n",
      "the time spent is 45.10409688949585\n",
      "0.5\n",
      "the time spent is 56.72283935546875\n",
      "the result 2.8b time is  [5.494949817657471, 11.137060403823853, 22.60603427886963, 33.62363815307617, 45.10409688949585, 56.72283935546875]\n",
      "the result 2.8b  [0.95, 0.975, 0.9, 0.825, 0.7, 0.5]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "mamba_2_8b_tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-2.8b-hf\", padding_side='left', cache_dir=\"./mamba-2.8b-hf\")\n",
    "mamba_2_8b_model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-2.8b-hf\", cache_dir=\"./mamba-2.8b-hf\")\n",
    "mamba_2_8b_model.to(device)\n",
    "\n",
    "import time \n",
    "result_2_8b_time = []\n",
    "result_2_8b = []\n",
    "for i in [25, 50, 100, 150, 200, 250]:\n",
    "    start_time = time.time()\n",
    "    ans = copy_task(batch_size = 4, model = mamba_2_8b_model, tokenizer = mamba_370m_tokenizer, token_max_len=i)\n",
    "    end_time = time.time()\n",
    "    result_2_8b_time.append(end_time - start_time)\n",
    "    result_2_8b.append(ans)\n",
    "    print(ans)\n",
    "    print('the time spent is', end_time - start_time)\n",
    "print(\"the result 2.8b time is \", result_2_8b_time)\n",
    "print(\"the result 2.8b \", result_2_8b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "name_phone_pairs = []\n",
    "with open('./phonebook.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line[-1] == ',':\n",
    "            line = line[:-1]\n",
    "        pair = ast.literal_eval(line)\n",
    "        name_phone_pairs.append((pair[0], pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We found the phone book experiment hard to reproduce as the author did not give the exact prompt in the paper. \n",
    "# In addition, the accuracy fluctutaed a lot with the prompt we used.\n",
    "\n",
    "import random\n",
    "def phone_book_task(batch_size=64, batches=10, book_size=20, model=None, tokenizer=None):\n",
    "    book = ''\n",
    "    success_lookups = 0\n",
    "    for i in range(book_size):\n",
    "        name = name_phone_pairs[i][0]\n",
    "        phone = name_phone_pairs[i][1]\n",
    "        book = book + name + ': ' + phone + '.\\n'\n",
    "    book += 'Extract the person\\'s phone number in the phonebook above. For example:\\nPerson: Liam\\nNumber: 436-725-2906\\nPerson: Olivia\\nNumber: 192-311-5790\\n\\n'\n",
    "    for _ in range(batches):\n",
    "        cur_batch = []\n",
    "        gold_num_tokens_batch = []\n",
    "        max_num_tokens = -1\n",
    "        for _ in range(batch_size):\n",
    "            query_pair_idx = random.randint(2, book_size)\n",
    "            query = book + 'Person: ' + name_phone_pairs[query_pair_idx][0] + '\\nNumber:'\n",
    "            gold_num_tokens = tokenizer(name_phone_pairs[query_pair_idx][1], return_tensors=\"pt\", padding=True).to(device)[\"input_ids\"]\n",
    "            max_num_tokens = max(max_num_tokens, gold_num_tokens.shape[1])\n",
    "            gold_num_tokens_batch.append(gold_num_tokens[0])\n",
    "            cur_batch.append(query)\n",
    "        input_ids = tokenizer(cur_batch, return_tensors=\"pt\", padding=True).to(device)[\"input_ids\"]\n",
    "        output_ids = model.generate(input_ids, max_new_tokens = max_num_tokens)\n",
    "        for count in range(batch_size):\n",
    "            true_number = tokenizer.decode(gold_num_tokens_batch[count])\n",
    "            output_answer = tokenizer.decode(output_ids[count])\n",
    "            if output_answer.count(true_number) > 1:\n",
    "                success_lookups += 1\n",
    "    return success_lookups / (batch_size * batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 348.00 MiB. GPU 0 has a total capacity of 14.58 GiB of which 335.56 MiB is free. Including non-PyTorch memory, this process has 14.25 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 193.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m t1 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(phone_book_task(model\u001b[39m=\u001b[39;49mmamba_370m_model, tokenizer\u001b[39m=\u001b[39;49mmamba_370m_tokenizer))\n\u001b[1;32m      3\u001b[0m t2 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(phone_book_task(model\u001b[39m=\u001b[39mmamba_370m_model, tokenizer\u001b[39m=\u001b[39mmamba_370m_tokenizer, book_size\u001b[39m=\u001b[39m\u001b[39m40\u001b[39m))\n",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m, in \u001b[0;36mphone_book_task\u001b[0;34m(batch_size, batches, book_size, model, tokenizer)\u001b[0m\n\u001b[1;32m     23\u001b[0m     cur_batch\u001b[39m.\u001b[39mappend(query)\n\u001b[1;32m     24\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer(cur_batch, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mto(device)[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 25\u001b[0m output_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids, max_new_tokens \u001b[39m=\u001b[39;49m max_num_tokens)\n\u001b[1;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m count \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_size):\n\u001b[1;32m     27\u001b[0m     true_number \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(gold_num_tokens_batch[count])\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/transformers/generation/utils.py:1531\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massisted_decoding(\n\u001b[1;32m   1515\u001b[0m         input_ids,\n\u001b[1;32m   1516\u001b[0m         candidate_generator\u001b[39m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1527\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1528\u001b[0m     )\n\u001b[1;32m   1529\u001b[0m \u001b[39mif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1530\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1531\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_greedy_search(\n\u001b[1;32m   1532\u001b[0m         input_ids,\n\u001b[1;32m   1533\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mprepared_logits_processor,\n\u001b[1;32m   1534\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mprepared_stopping_criteria,\n\u001b[1;32m   1535\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1536\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1537\u001b[0m         output_logits\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_logits,\n\u001b[1;32m   1538\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1539\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1540\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1541\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1542\u001b[0m     )\n\u001b[1;32m   1544\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1545\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/transformers/generation/utils.py:2436\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2433\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2435\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2436\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2437\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2438\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2439\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2440\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2441\u001b[0m )\n\u001b[1;32m   2443\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2444\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/transformers/models/mamba/modeling_mamba.py:661\u001b[0m, in \u001b[0;36mMambaForCausalLM.forward\u001b[0;34m(self, input_ids, inputs_embeds, cache_params, labels, output_hidden_states, return_dict, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    659\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 661\u001b[0m mamba_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(\n\u001b[1;32m    662\u001b[0m     input_ids,\n\u001b[1;32m    663\u001b[0m     cache_params\u001b[39m=\u001b[39;49mcache_params,\n\u001b[1;32m    664\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    665\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    666\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    667\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    668\u001b[0m )\n\u001b[1;32m    669\u001b[0m hidden_states \u001b[39m=\u001b[39m mamba_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    671\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdtype))\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/transformers/models/mamba/modeling_mamba.py:563\u001b[0m, in \u001b[0;36mMambaModel.forward\u001b[0;34m(self, input_ids, inputs_embeds, cache_params, use_cache, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(mixer_block\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m, hidden_states, cache_params)\n\u001b[1;32m    562\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 563\u001b[0m     hidden_states \u001b[39m=\u001b[39m mixer_block(hidden_states, cache_params\u001b[39m=\u001b[39;49mcache_params)\n\u001b[1;32m    565\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    566\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/transformers/models/mamba/modeling_mamba.py:328\u001b[0m, in \u001b[0;36mMambaBlock.forward\u001b[0;34m(self, hidden_states, cache_params)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresidual_in_fp32:\n\u001b[1;32m    326\u001b[0m     residual \u001b[39m=\u001b[39m residual\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m--> 328\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmixer(hidden_states, cache_params\u001b[39m=\u001b[39;49mcache_params)\n\u001b[1;32m    329\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    330\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/transformers/models/mamba/modeling_mamba.py:292\u001b[0m, in \u001b[0;36mMambaMixer.forward\u001b[0;34m(self, hidden_states, cache_params)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states, cache_params: Optional[MambaCache] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    291\u001b[0m     \u001b[39mif\u001b[39;00m is_fast_path_available \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_proj\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype:\n\u001b[0;32m--> 292\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcuda_kernels_forward(hidden_states, cache_params)\n\u001b[1;32m    293\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslow_forward(hidden_states, cache_params)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/transformers/models/mamba/modeling_mamba.py:204\u001b[0m, in \u001b[0;36mMambaMixer.cuda_kernels_forward\u001b[0;34m(self, hidden_states, cache_params)\u001b[0m\n\u001b[1;32m    191\u001b[0m     scan_outputs \u001b[39m=\u001b[39m selective_state_update(\n\u001b[1;32m    192\u001b[0m         cache_params\u001b[39m.\u001b[39mssm_states[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_idx],\n\u001b[1;32m    193\u001b[0m         hidden_states[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m         dt_softplus\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    202\u001b[0m     )\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    203\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m     scan_outputs, ssm_state \u001b[39m=\u001b[39m selective_scan_fn(\n\u001b[1;32m    205\u001b[0m         hidden_states,\n\u001b[1;32m    206\u001b[0m         discrete_time_step,\n\u001b[1;32m    207\u001b[0m         A,\n\u001b[1;32m    208\u001b[0m         B\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m),\n\u001b[1;32m    209\u001b[0m         C\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m),\n\u001b[1;32m    210\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mD\u001b[39m.\u001b[39;49mfloat(),\n\u001b[1;32m    211\u001b[0m         gate,\n\u001b[1;32m    212\u001b[0m         time_proj_bias,\n\u001b[1;32m    213\u001b[0m         delta_softplus\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    214\u001b[0m         return_last_state\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    215\u001b[0m     )\n\u001b[1;32m    216\u001b[0m     \u001b[39mif\u001b[39;00m ssm_state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m cache_params \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         cache_params\u001b[39m.\u001b[39mssm_states[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_idx]\u001b[39m.\u001b[39mcopy_(ssm_state)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/mamba_ssm/ops/selective_scan_interface.py:88\u001b[0m, in \u001b[0;36mselective_scan_fn\u001b[0;34m(u, delta, A, B, C, D, z, delta_bias, delta_softplus, return_last_state)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselective_scan_fn\u001b[39m(u, delta, A, B, C, D\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, z\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, delta_bias\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, delta_softplus\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     83\u001b[0m                      return_last_state\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     84\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"if return_last_state is True, returns (out, last_state)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39m    last_state has shape (batch, dim, dstate). Note that the gradient of the last state is\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[39m    not considered in the backward pass.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m SelectiveScanFn\u001b[39m.\u001b[39;49mapply(u, delta, A, B, C, D, z, delta_bias, delta_softplus, return_last_state)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_out/lib/python3.10/site-packages/mamba_ssm/ops/selective_scan_interface.py:35\u001b[0m, in \u001b[0;36mSelectiveScanFn.forward\u001b[0;34m(ctx, u, delta, A, B, C, D, z, delta_bias, delta_softplus, return_last_state)\u001b[0m\n\u001b[1;32m     33\u001b[0m     C \u001b[39m=\u001b[39m C\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m z \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m z\u001b[39m.\u001b[39mstride(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m     z \u001b[39m=\u001b[39m z\u001b[39m.\u001b[39;49mcontiguous()\n\u001b[1;32m     36\u001b[0m \u001b[39mif\u001b[39;00m B\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m     37\u001b[0m     B \u001b[39m=\u001b[39m rearrange(B, \u001b[39m\"\u001b[39m\u001b[39mb dstate l -> b 1 dstate l\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 348.00 MiB. GPU 0 has a total capacity of 14.58 GiB of which 335.56 MiB is free. Including non-PyTorch memory, this process has 14.25 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 193.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "print(phone_book_task(model=mamba_370m_model, tokenizer=mamba_370m_tokenizer))\n",
    "t2 = time.time()\n",
    "print(phone_book_task(model=mamba_370m_model, tokenizer=mamba_370m_tokenizer, book_size=40))\n",
    "t3 = time.time()\n",
    "print(phone_book_task(batch_size=32, batches=20, model=mamba_370m_model, tokenizer=mamba_370m_tokenizer, book_size=80))\n",
    "t4 = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[137.03576588630676, 248.3478810787201, 469.9964528083801]\n"
     ]
    }
   ],
   "source": [
    "print([t2-t1, t3-t2, t4-t3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [44.008848905563354, 76.88284754753113, 148.7303111553192] for 370\n",
    "# [137.03576588630676, 248.3478810787201, 469.9964528083801] for 1.4b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba_out",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c744e102f8b68e13ccb97907c8e884f7b13598111d445b31ba6335dfb2acb9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
