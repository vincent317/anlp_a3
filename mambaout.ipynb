{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "def load_json_gz(filename):\n",
    "    with gzip.open(filename, 'r') as f:\n",
    "        i = 0\n",
    "        ret = []\n",
    "        for json_line in f:\n",
    "            if i == 10000:\n",
    "                return ret\n",
    "            data = json.loads(json_line)\n",
    "            text = data['text']\n",
    "            if len(text) > 2000:\n",
    "                ret.append(text)\n",
    "                i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now download the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 10000 strings from C4 dataset: https://huggingface.co/datasets/allenai/c4/tree/main/en\n",
    "strings = load_json_gz('c4-train.00000-of-01024.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mamba_out/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_task(batch_size=64, batches=10, model=None, tokenizer=None, token_max_len=25, shuffle=False):\n",
    "    string_idx = 0\n",
    "    success_copies = 0\n",
    "    for _ in range(batches):\n",
    "        cur_batch = []\n",
    "        for count in range(batch_size):\n",
    "            cur_batch.append(strings[count + string_idx])\n",
    "        input_ids = tokenizer(cur_batch, return_tensors=\"pt\", truncation=True, max_length=token_max_len).to(device)[\"input_ids\"]\n",
    "        if shuffle:\n",
    "            col_perm = torch.randperm(input_ids.size(1))\n",
    "            input_ids = input_ids[:, col_perm]\n",
    "        input_ids = torch.cat([input_ids, input_ids], dim=1)\n",
    "        input_ids = torch.cat([input_ids, input_ids[:, 0:1]], dim=1)\n",
    "        output_ids = model.generate(input_ids, max_new_tokens = token_max_len-1)\n",
    "        for count in range(batch_size):\n",
    "            gold_token_len = (input_ids.shape[1]-1) // 2\n",
    "            if torch.equal(input_ids[count][:gold_token_len], output_ids[count][gold_token_len*2:]):\n",
    "                success_copies += 1\n",
    "        string_idx += batch_size\n",
    "    return success_copies / (batch_size * batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "the time spent is 10.187240600585938\n",
      "0.6625\n",
      "the time spent is 19.774027585983276\n",
      "0.5\n",
      "the time spent is 40.74758529663086\n",
      "0.4\n",
      "the time spent is 62.89952063560486\n",
      "0.2\n",
      "the time spent is 80.89390206336975\n",
      "0.1125\n",
      "the time spent is 101.32650256156921\n",
      "the result 370m time is  [10.187240600585938, 19.774027585983276, 40.74758529663086, 62.89952063560486, 80.89390206336975, 101.32650256156921]\n",
      "the result 370m is  [0.9, 0.6625, 0.5, 0.4, 0.2, 0.1125]\n"
     ]
    }
   ],
   "source": [
    "mamba_370m_tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-370m-hf\")\n",
    "mamba_370m_model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-370m-hf\")\n",
    "mamba_370m_model.to(device)\n",
    "import time \n",
    "result_370m_time = []\n",
    "result_370m = []\n",
    "for i in [25, 50, 100, 150, 200, 250]:\n",
    "    start_time = time.time()\n",
    "    ans = copy_task(batch_size = 8, model = mamba_370m_model, tokenizer = mamba_370m_tokenizer, token_max_len=i)\n",
    "    end_time = time.time()\n",
    "    result_370m_time.append(end_time - start_time)\n",
    "    result_370m.append(ans)\n",
    "    print(ans)\n",
    "    print('the time spent is', end_time - start_time)\n",
    "\n",
    "print(\"the result 370m time is \", result_370m_time)\n",
    "print(\"the result 370m is \", result_370m)\n",
    "# print(copy_task(model=mamba_370m_model, tokenizer=mamba_370m_tokenizer, token_max_len=25))\n",
    "# print(copy_task(model=mamba_370m_model, tokenizer=mamba_370m_tokenizer, token_max_len=50))\n",
    "# print(copy_task(model=mamba_370m_model, tokenizer=mamba_370m_tokenizer, token_max_len=100))\n",
    "# print(copy_task(model=mamba_370m_model, tokenizer=mamba_370m_tokenizer, token_max_len=150))\n",
    "# print(copy_task(model=mamba_370m_model, tokenizer=mamba_370m_tokenizer, token_max_len=200))\n",
    "# print(copy_task(model=mamba_370m_model, tokenizer=mamba_370m_tokenizer, token_max_len=250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9375\n",
      "the time spent is 9.845363855361938\n",
      "0.55\n",
      "the time spent is 19.699750900268555\n",
      "0.075\n",
      "the time spent is 40.13555192947388\n",
      "0.0\n",
      "the time spent is 60.27432703971863\n",
      "0.0\n",
      "the time spent is 80.2201087474823\n",
      "0.0\n",
      "the time spent is 100.15279531478882\n"
     ]
    }
   ],
   "source": [
    "result_370m_shuffle_time = []\n",
    "result_370m_shuffle = []\n",
    "for i in [25, 50, 100, 150, 200, 250]:\n",
    "    start_time = time.time()\n",
    "    ans = copy_task(batch_size = 8, model = mamba_370m_model, tokenizer = mamba_370m_tokenizer, token_max_len=i, shuffle=True)\n",
    "    end_time = time.time()\n",
    "    result_370m_shuffle_time.append(end_time - start_time)\n",
    "    result_370m_shuffle.append(ans)\n",
    "    print(ans)\n",
    "    print('the time spent is', end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shuffle 370m results [0.9375, 0.55, 0.075, 0.0, 0.0, 0.0]\n",
      "the shuffle 370 time [9.845363855361938, 19.699750900268555, 40.13555192947388, 60.27432703971863, 80.2201087474823, 100.15279531478882]\n"
     ]
    }
   ],
   "source": [
    "print(\"the shuffle 370m results\", result_370m_shuffle)\n",
    "print(\"the shuffle 370 time\", result_370m_shuffle_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857f5f36675a4708ace203ea42f7414a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n",
      "the time spent is 4.088669776916504\n",
      "0.85\n",
      "the time spent is 8.237766981124878\n",
      "0.825\n",
      "the time spent is 16.61814284324646\n",
      "0.65\n",
      "the time spent is 24.92847204208374\n",
      "0.45\n",
      "the time spent is 33.319599628448486\n",
      "0.3\n",
      "the time spent is 40.86095356941223\n",
      "the result 1.4b time is  [4.088669776916504, 8.237766981124878, 16.61814284324646, 24.92847204208374, 33.319599628448486, 40.86095356941223]\n",
      "the result 1.4b  [0.95, 0.85, 0.825, 0.65, 0.45, 0.3]\n"
     ]
    }
   ],
   "source": [
    "mamba_1_4b_tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-1.4b-hf\")\n",
    "mamba_1_4b_model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-1.4b-hf\")\n",
    "mamba_1_4b_model.to(device)\n",
    "\n",
    "import time \n",
    "result_1_4b_time = []\n",
    "result_1_4b = []\n",
    "for i in [25, 50, 100, 150, 200, 250]:\n",
    "    start_time = time.time()\n",
    "    ans = copy_task(batch_size = 4, model = mamba_1_4b_model, tokenizer = mamba_1_4b_tokenizer, token_max_len=i)\n",
    "    end_time = time.time()\n",
    "    result_1_4b_time.append(end_time - start_time)\n",
    "    result_1_4b.append(ans)\n",
    "    print(ans)\n",
    "    print('the time spent is', end_time - start_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the result 1.4b time is  [4.088669776916504, 8.237766981124878, 16.61814284324646, 24.92847204208374, 33.319599628448486, 40.86095356941223]\n",
      "the result 1.4b  [0.95, 0.85, 0.825, 0.65, 0.45, 0.3]\n"
     ]
    }
   ],
   "source": [
    "print(\"the result 1.4b time is \", result_1_4b_time)\n",
    "print(\"the result 1.4b \", result_1_4b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b7ba9773374997a4ecade93909d83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0536ba8e82c547579979cc5cc605e1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0b32ffdb554642b1ff78a4a232e71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3353f1421e74c98ae1d2c26b43d811b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/50.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f75d734682140fcbb11c35aee58f56c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48bdaa0b26c401f89e0c4dfcfd34b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1d8722068343abb514e4fdde5032ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b933fc9e9ea49288ca5d3ad21ec0606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/1.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55b52206a8d45ac826fa04259609099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcc701b3c3a44c49481f4b60a2fa566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n",
      "the time spent is 5.494949817657471\n",
      "0.975\n",
      "the time spent is 11.137060403823853\n",
      "0.9\n",
      "the time spent is 22.60603427886963\n",
      "0.825\n",
      "the time spent is 33.62363815307617\n",
      "0.7\n",
      "the time spent is 45.10409688949585\n",
      "0.5\n",
      "the time spent is 56.72283935546875\n",
      "the result 2.8b time is  [5.494949817657471, 11.137060403823853, 22.60603427886963, 33.62363815307617, 45.10409688949585, 56.72283935546875]\n",
      "the result 2.8b  [0.95, 0.975, 0.9, 0.825, 0.7, 0.5]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "mamba_2_8b_tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-2.8b-hf\", padding_side='left', cache_dir=\"./mamba-2.8b-hf\")\n",
    "mamba_2_8b_model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-2.8b-hf\", cache_dir=\"./mamba-2.8b-hf\")\n",
    "mamba_2_8b_model.to(device)\n",
    "\n",
    "import time \n",
    "result_2_8b_time = []\n",
    "result_2_8b = []\n",
    "for i in [25, 50, 100, 150, 200, 250]:\n",
    "    start_time = time.time()\n",
    "    ans = copy_task(batch_size = 4, model = mamba_2_8b_model, tokenizer = mamba_370m_tokenizer, token_max_len=i)\n",
    "    end_time = time.time()\n",
    "    result_2_8b_time.append(end_time - start_time)\n",
    "    result_2_8b.append(ans)\n",
    "    print(ans)\n",
    "    print('the time spent is', end_time - start_time)\n",
    "print(\"the result 2.8b time is \", result_2_8b_time)\n",
    "print(\"the result 2.8b \", result_2_8b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "name_phone_pairs = []\n",
    "with open('./phonebook.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line[-1] == ',':\n",
    "            line = line[:-1]\n",
    "        pair = ast.literal_eval(line)\n",
    "        name_phone_pairs.append((pair[0], pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# We found the phone book experiment hard to reproduce as the author did not give the exact prompt in the paper. \n",
    "# In addition, the accuracy fluctutaed a lot with the prompt we used.\n",
    "\n",
    "import random\n",
    "def phone_book_task(batch_size=64, batches=10, book_size=20, model=None, tokenizer=None):\n",
    "    book = ''\n",
    "    success_lookups = 0\n",
    "    for i in range(book_size):\n",
    "        name = name_phone_pairs[i][0]\n",
    "        phone = name_phone_pairs[i][1]\n",
    "        book = book + name + ': ' + phone + '.\\n'\n",
    "    book += 'Extract the person\\'s phone number in the phonebook above. For example:\\nPerson: Liam\\nNumber: 436-725-2906\\nPerson: Olivia\\nNumber: 192-311-5790\\n\\n'\n",
    "    for _ in range(batches):\n",
    "        cur_batch = []\n",
    "        gold_num_tokens_batch = []\n",
    "        max_num_tokens = -1\n",
    "        for _ in range(batch_size):\n",
    "            query_pair_idx = random.randint(2, book_size)\n",
    "            query = book + 'Person: ' + name_phone_pairs[query_pair_idx][0] + '\\nNumber:'\n",
    "            gold_num_tokens = tokenizer(name_phone_pairs[query_pair_idx][1], return_tensors=\"pt\", padding=True).to(device)[\"input_ids\"]\n",
    "            max_num_tokens = max(max_num_tokens, gold_num_tokens.shape[1])\n",
    "            gold_num_tokens_batch.append(gold_num_tokens[0])\n",
    "            cur_batch.append(query)\n",
    "        input_ids = tokenizer(cur_batch, return_tensors=\"pt\", padding=True).to(device)[\"input_ids\"]\n",
    "        output_ids = model.generate(input_ids, max_new_tokens = max_num_tokens)\n",
    "        for count in range(batch_size):\n",
    "            true_number = tokenizer.decode(gold_num_tokens_batch[count])\n",
    "            output_answer = tokenizer.decode(output_ids[count])\n",
    "            if output_answer.count(true_number) > 1:\n",
    "                success_lookups += 1\n",
    "    return success_lookups / (batch_size * batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtqdm\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(phone_book_task(model\u001b[39m=\u001b[39;49mmamba_370m_model, tokenizer\u001b[39m=\u001b[39;49mmamba_370m_tokenizer))\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(phone_book_task(model\u001b[39m=\u001b[39mmamba_370m_model, tokenizer\u001b[39m=\u001b[39mmamba_370m_tokenizer, book_size\u001b[39m=\u001b[39m\u001b[39m40\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(phone_book_task(batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, batches\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, model\u001b[39m=\u001b[39mmamba_370m_model, tokenizer\u001b[39m=\u001b[39mmamba_370m_tokenizer, book_size\u001b[39m=\u001b[39m\u001b[39m80\u001b[39m))\n",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m, in \u001b[0;36mphone_book_task\u001b[0;34m(batch_size, batches, book_size, model, tokenizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m     book \u001b[39m=\u001b[39m book \u001b[39m+\u001b[39m name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m phone \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m     12\u001b[0m book \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mExtract the person\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39ms phone number in the phonebook above. For example:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mPerson: Liam\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mNumber: 436-725-2906\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mPerson: Olivia\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mNumber: 192-311-5790\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39;49m(batches)):\n\u001b[1;32m     14\u001b[0m     cur_batch \u001b[39m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m     gold_num_tokens_batch \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "print(phone_book_task(model=mamba_370m_model, tokenizer=mamba_370m_tokenizer))\n",
    "t2 = time.time()\n",
    "print(phone_book_task(model=mamba_370m_model, tokenizer=mamba_370m_tokenizer, book_size=40))\n",
    "t3 = time.time()\n",
    "print(phone_book_task(batch_size=32, batches=20, model=mamba_370m_model, tokenizer=mamba_370m_tokenizer, book_size=80))\n",
    "t4 = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba_out",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c744e102f8b68e13ccb97907c8e884f7b13598111d445b31ba6335dfb2acb9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
